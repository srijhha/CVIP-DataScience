import pandas as pd
import matplotlib.pyplot as plt
import string
import re
from collections import Counter
from nltk.corpus import stopwords
import seaborn as sns

from wordcloud import WordCloud

# Load the email dataset
emails = pd.read_csv('dataset.csv', encoding="ISO-8859-1")

# Data Exploration
print("Data Description:")
print(emails.describe())

# Check the shape of the dataset
print("Data Shape:", emails.shape)

# Display the first few rows of the dataset
print("First Few Rows:")
print(emails.head())

# Rename columns for clarity
emails = emails.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
emails = emails.rename(columns={"v1": "Classify", "v2": "Email"})

print("Modified Data:")
print(emails.head())

# Check for missing data
print("Missing Data Summary:")
print(emails.isnull().sum())

# Remove duplicate entries
duplicates_before = emails.duplicated().sum()
emails = emails.drop_duplicates()
duplicates_after = emails.duplicated().sum()
print("Removed Duplicates:", duplicates_before - duplicates_after)

# Calculate and visualize the percentage of spam and ham emails
spam_count = len(emails[emails['Classify'] == 'spam'])
ham_count = len(emails[emails['Classify'] == 'ham'])
total_messages = len(emails)
spam_percentage = (spam_count / total_messages) * 100
ham_percentage = (ham_count / total_messages) * 100

# Create a pie chart to visualize the spam vs. ham percentage
labels = ['Spam', 'Regular (ham)']
sizes = [spam_percentage, ham_percentage]
colors = ['#ff9999', '#66b3ff']
explode = (0.1, 0)

plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Percentage of Spam Messages to Regular Messages')
plt.axis('equal')
plt.show()

# Data Preprocessing
# Remove punctuation marks
emails['Email'] = emails['Email'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))

# Convert email text to lowercase
emails['Email'] = emails['Email'].str.lower()

# Remove stopwords
stop_words = set(stopwords.words('english'))
emails['Email'] = emails['Email'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Word Frequency Analysis
all_words = ' '.join(emails['Email']).split()
word_counts = Counter(all_words)
most_common_words = word_counts.most_common(10)

print("Top 10 Most Common Words:")
print(most_common_words)

# Create a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white')
wordcloud.generate_from_frequencies(dict(most_common_words))

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - Top 10 Most Common Words')
plt.show()

# Phone Number Detection in Spam
phone_number_count = 0

for message in emails.loc[emails['Classify'] == 'spam', 'Email']:
    if re.search(r'\b\d{10}\b', message):  # assuming phone numbers are 10 digits long
        phone_number_count += 1

print("Number of spam messages containing a phone number:", phone_number_count)

# Distribution of Message Length
emails['number_of_characters_in_the_message'] = emails['Email'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(emails['number_of_characters_in_the_message'], bins=50, kde=True)
plt.xlabel('Number of Characters in the Message')
plt.ylabel('Frequency')
plt.title('Distribution of Message Length')
plt.show()
